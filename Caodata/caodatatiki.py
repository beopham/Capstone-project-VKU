import csv
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import os

# --- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ƒê√É S·ª¨A CH·ªÆA (T∆∞∆°ng th√≠ch v·ªõi c·∫•u tr√∫c c·ªßa b·∫°n) ---

# L·∫•y ƒë∆∞·ªùng d·∫´n c·ªßa th∆∞ m·ª•c Caodata (th∆∞ m·ª•c ch·ª©a file code hi·ªán t·∫°i)
CAODATA_DIR = os.path.dirname(os.path.abspath(__file__))

# L√πi 1 c·∫•p th∆∞ m·ª•c ƒë·ªÉ tr·ªè v·ªÅ th∆∞ m·ª•c g·ªëc c·ªßa project (Capstone-project-VKU)
BASE_PROJECT_DIR = os.path.dirname(CAODATA_DIR)

# 1. ƒê∆∞·ªùng d·∫´n Driver (N·∫±m ngo√†i Caodata)
DRIVER_PATH = os.path.join(BASE_PROJECT_DIR, "chromedriver", "chromedriver.exe")

# 2. ƒê∆∞·ªùng d·∫´n CSV (N·∫±m trong File_Saved)
CSV_PATH = os.path.join(CAODATA_DIR, "File_Saved", "tiki_reviews.csv")


def scroll_to_bottom(driver, pause_time=1, max_scrolls=10):
    """Cu·ªôn trang xu·ªëng nhi·ªÅu l·∫ßn ƒë·ªÉ t·∫£i t·∫•t c·∫£ ƒë√°nh gi√°."""
    last_height = driver.execute_script("return document.body.scrollHeight")
    for i in range(max_scrolls):
        driver.execute_script("window.scrollBy(0, 1000);")
        time.sleep(pause_time)
        driver.execute_script("window.scrollBy(0, window.innerHeight);")
        time.sleep(pause_time)

        new_height = driver.execute_script("return document.body.scrollHeight")
        print(f"  > ƒêang cu·ªôn: {i + 1}/{max_scrolls}...")

        if new_height == last_height:
            break
        last_height = new_height


def get_reviews_from_pages(driver, max_pages=10):
    """Thu th·∫≠p ƒë√°nh gi√°, t·ª± ƒë·ªông chuy·ªÉn trang."""
    all_reviews = []
    wait = WebDriverWait(driver, 5)

    for page in range(1, max_pages + 1):
        print(f"  > ƒêang l·∫•y ƒë√°nh gi√° trang {page}...")

        try:
            wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, "review-comment__content")))
            comments = driver.find_elements(By.CLASS_NAME, "review-comment__content")
            all_reviews.extend([c.text for c in comments if c.text.strip()])
        except Exception:
            print("  > Kh√¥ng t√¨m th·∫•y ƒë√°nh gi√° n√†o tr√™n trang n√†y.")

        next_page_num = page + 1
        try:
            next_btn = wait.until(
                EC.element_to_be_clickable((By.XPATH, f"//a[text()='{next_page_num}']"))
            )
            next_btn.click()
            time.sleep(2)
        except Exception:
            print("  > H·∫øt trang ho·∫∑c kh√¥ng c√≥ n√∫t k·∫ø ti·∫øp.")
            break

    return all_reviews


def getProductInfo(driver, url):
    """Truy c·∫≠p URL, l·∫•y th√¥ng tin s·∫£n ph·∫©m v√† ƒë√°nh gi√°, ghi v√†o CSV."""
    print("-" * 50)
    print(f"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω URL: {url}")
    driver.get(url)

    wait = WebDriverWait(driver, 10)

    try:
        title = wait.until(EC.presence_of_element_located((By.TAG_NAME, "h1"))).text
    except Exception:
        title = "Kh√¥ng t√¨m th·∫•y t√™n s·∫£n ph·∫©m"

    try:
        price = wait.until(EC.presence_of_element_located((By.CLASS_NAME, "product-price__current-price"))).text
    except Exception:
        price = "Kh√¥ng t√¨m th·∫•y gi√° s·∫£n ph·∫©m"

    print(f"\nüì¶ S·∫£n ph·∫©m: {title}")
    print(f"üí∞ Gi√°: {price}")

    scroll_to_bottom(driver)
    reviews = get_reviews_from_pages(driver)

    print(f"üìù Thu th·∫≠p ƒë∆∞·ª£c {len(reviews)} ƒë√°nh gi√°.")

    # Ki·ªÉm tra v√† t·∫°o th∆∞ m·ª•c File_Saved n·∫øu ch∆∞a t·ªìn t·∫°i
    save_dir = os.path.dirname(CSV_PATH)
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
        print(f"T·∫°o th∆∞ m·ª•c: {save_dir}")

    # Ghi v√†o file CSV
    file_exists = os.path.exists(CSV_PATH) and os.path.getsize(CSV_PATH) > 0
    with open(CSV_PATH, mode="a", encoding="utf-8", newline="") as file:
        writer = csv.writer(file)
        if not file_exists:
            writer.writerow(["T√™n s·∫£n ph·∫©m", "Gi√°", "ƒê√°nh gi√°"])
        for review in reviews:
            writer.writerow([title, price, review])

    print(f"‚úÖ ƒê√£ ghi {len(reviews)} ƒë√°nh gi√° v√†o file: {CSV_PATH}")


def startBot(urls):
    """Kh·ªüi ƒë·ªông bot v√† x·ª≠ l√Ω danh s√°ch URL."""
    driver = None
    try:
        if not os.path.exists(DRIVER_PATH):
            print(f"L·ªñI: Kh√¥ng t√¨m th·∫•y chromedriver.exe t·∫°i: {DRIVER_PATH}")
            print("‚Üí H√£y ki·ªÉm tra l·∫°i c·∫•u tr√∫c th∆∞ m·ª•c (n√≥ ph·∫£i n·∫±m trong Capstone-project-VKU/chromedriver).")
            return

        service = Service(DRIVER_PATH)
        driver = webdriver.Chrome(service=service)

        for url in urls:
            getProductInfo(driver, url)

    except Exception as e:
        print(f"ƒê√£ x·∫£y ra l·ªói: {e}")

    finally:
        if driver:
            driver.quit()


# ----------------------------------------------------------------------------------
# DANH S√ÅCH URL S·∫¢N PH·∫®M
# ----------------------------------------------------------------------------------
product_urls = [
    "https://tiki.vn/ta-bim-quan-huggies-skin-care-mega-jumbo-xl84-4-mieng-voi-tram-tra-diu-da-p275220816.html?itm_campaign=CTP_YPD_TKA_PLA_UNK_ALL_UNK_UNK_UNK_UNK_X.308555_Y.1890875_Z.4030442_CN.Key-ta-quan-Skin-Care&itm_medium=CPC&itm_source=tiki-ads&spid=275220817",
]

# ----------------------------------------------------------------------------------
# KH·ªûI CH·∫†Y CH∆Ø∆†NG TR√åNH
# ----------------------------------------------------------------------------------
if __name__ == "__main__":
    startBot(product_urls)